{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ad84fe-e19d-40a8-9782-ea5901a267d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive namespace is empty.\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18f1d616-0b3d-48b5-95a5-bf6044a3f816",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (2392556223.py, line 131)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [4]\u001b[0;36m\u001b[0m\n\u001b[0;31m    #embedding = pd.DataFrame(embedding)\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        \n",
    "        self.encoder: Model = encoder\n",
    "        self.decoder: Model = decoder\n",
    "        \n",
    "        self.total_loss_tracker = keras.metrics.Mean(name = \"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name = \"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name = \"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss_fn = keras.losses.MeanSquaredError()\n",
    "            reconstruction_loss = reconstruction_loss_fn(data, reconstruction)\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        return self.decoder(z)\n",
    "\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "latent_dim = 10\n",
    "base_path = 'rvrs_out'\n",
    "\n",
    "X, y = make_classification(n_samples=1000,\n",
    "                           n_features=100,\n",
    "                           n_informative=10,\n",
    "                           n_redundant=90,\n",
    "                           random_state=1,\n",
    "                           shift = 30)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler = scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "data = X\n",
    "\n",
    "data = np.trunc(1000 * data) / 1000\n",
    "\n",
    "X_train, X_val = train_test_split(data, test_size=0.2)\n",
    "\n",
    "train_data = X_train\n",
    "val_data = X_val\n",
    "\n",
    "input_dimensions = train_data.shape[1]\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(input_dimensions,))\n",
    "x = layers.Dense(units=input_dimensions / 2, activation=\"relu\")(encoder_inputs)\n",
    "x = layers.Dense(units=input_dimensions / 3, activation=\"relu\")(x)\n",
    "x = layers.Dense(units=input_dimensions / 4, activation=\"relu\")(x)\n",
    "\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()\n",
    "\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(units=input_dimensions / 4, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Dense(units=input_dimensions / 3, activation=\"relu\")(x)\n",
    "x = layers.Dense(units=input_dimensions / 2, activation=\"relu\")(x)\n",
    "\n",
    "decoder_outputs = layers.Dense(units=input_dimensions, activation=\"relu\")(x)\n",
    "\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()\n",
    "\n",
    "vae: VAE = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "callbacks = []\n",
    "early_stop = EarlyStopping(monitor=\"reconstruction_loss\",\n",
    "                           mode=\"min\", patience=5,\n",
    "                           restore_best_weights=True)\n",
    "\n",
    "callbacks.append(early_stop)\n",
    "csv_logger = CSVLogger(os.path.join(base_path, 'training.log'), # Is this writing a file?\n",
    "                       separator='\\t')\n",
    "callbacks.append(csv_logger)\n",
    "history = vae.fit(train_data, # Fit the VAE, work backward to carve out minimum viable code path to run this function\n",
    "                  callbacks=callbacks,\n",
    "                  validation_data=(val_data, val_data),\n",
    "z_mean, z_var, embedding = vae.encoder.predict(test_data)\n",
    "embedding = pd.DataFrame(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d40324b-cf97-4f12-8673-3e6970d2220b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7dd8dc7880>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3822f06-9cb4-43b0-946c-33a11b4fbe41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.021363</td>\n",
       "      <td>-1.199799</td>\n",
       "      <td>0.137270</td>\n",
       "      <td>1.297587</td>\n",
       "      <td>2.042372</td>\n",
       "      <td>0.183850</td>\n",
       "      <td>-0.775907</td>\n",
       "      <td>0.255613</td>\n",
       "      <td>0.981052</td>\n",
       "      <td>0.354788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.041011</td>\n",
       "      <td>-1.260385</td>\n",
       "      <td>0.132285</td>\n",
       "      <td>-0.479902</td>\n",
       "      <td>-0.364104</td>\n",
       "      <td>-0.319177</td>\n",
       "      <td>-0.984617</td>\n",
       "      <td>-0.083420</td>\n",
       "      <td>-0.033995</td>\n",
       "      <td>-0.696346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.444304</td>\n",
       "      <td>-0.689801</td>\n",
       "      <td>-0.093025</td>\n",
       "      <td>-1.255087</td>\n",
       "      <td>1.027433</td>\n",
       "      <td>0.396463</td>\n",
       "      <td>-0.138884</td>\n",
       "      <td>0.043872</td>\n",
       "      <td>-0.855598</td>\n",
       "      <td>-0.737397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.968805</td>\n",
       "      <td>-0.195319</td>\n",
       "      <td>-0.755937</td>\n",
       "      <td>0.917800</td>\n",
       "      <td>-0.407270</td>\n",
       "      <td>-0.048072</td>\n",
       "      <td>-0.746339</td>\n",
       "      <td>0.903619</td>\n",
       "      <td>0.502316</td>\n",
       "      <td>-0.904222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.050442</td>\n",
       "      <td>0.034416</td>\n",
       "      <td>1.446062</td>\n",
       "      <td>-0.247074</td>\n",
       "      <td>-0.749640</td>\n",
       "      <td>-0.040352</td>\n",
       "      <td>0.963695</td>\n",
       "      <td>1.084258</td>\n",
       "      <td>0.246785</td>\n",
       "      <td>0.879446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>-0.857996</td>\n",
       "      <td>1.304992</td>\n",
       "      <td>-1.216675</td>\n",
       "      <td>-0.690762</td>\n",
       "      <td>0.625938</td>\n",
       "      <td>0.556321</td>\n",
       "      <td>-0.468636</td>\n",
       "      <td>0.207434</td>\n",
       "      <td>-0.181016</td>\n",
       "      <td>-0.913800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>0.144425</td>\n",
       "      <td>-0.251678</td>\n",
       "      <td>-0.103680</td>\n",
       "      <td>-0.865499</td>\n",
       "      <td>-0.878160</td>\n",
       "      <td>0.570694</td>\n",
       "      <td>-1.595752</td>\n",
       "      <td>0.051418</td>\n",
       "      <td>-0.625263</td>\n",
       "      <td>-0.956271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>-0.589522</td>\n",
       "      <td>-0.880361</td>\n",
       "      <td>0.773587</td>\n",
       "      <td>-0.296867</td>\n",
       "      <td>-0.271078</td>\n",
       "      <td>-0.199294</td>\n",
       "      <td>-1.703998</td>\n",
       "      <td>1.282618</td>\n",
       "      <td>1.047622</td>\n",
       "      <td>2.104883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>-0.136441</td>\n",
       "      <td>0.646560</td>\n",
       "      <td>-0.476017</td>\n",
       "      <td>-0.062668</td>\n",
       "      <td>-0.901895</td>\n",
       "      <td>-1.633245</td>\n",
       "      <td>0.811345</td>\n",
       "      <td>0.688448</td>\n",
       "      <td>0.797758</td>\n",
       "      <td>0.536947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>-0.552745</td>\n",
       "      <td>-0.270062</td>\n",
       "      <td>0.355187</td>\n",
       "      <td>-1.202716</td>\n",
       "      <td>-1.357263</td>\n",
       "      <td>1.072818</td>\n",
       "      <td>0.115408</td>\n",
       "      <td>0.594687</td>\n",
       "      <td>1.557900</td>\n",
       "      <td>1.272627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0   -1.021363 -1.199799  0.137270  1.297587  2.042372  0.183850 -0.775907   \n",
       "1    1.041011 -1.260385  0.132285 -0.479902 -0.364104 -0.319177 -0.984617   \n",
       "2    0.444304 -0.689801 -0.093025 -1.255087  1.027433  0.396463 -0.138884   \n",
       "3    0.968805 -0.195319 -0.755937  0.917800 -0.407270 -0.048072 -0.746339   \n",
       "4    0.050442  0.034416  1.446062 -0.247074 -0.749640 -0.040352  0.963695   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "795 -0.857996  1.304992 -1.216675 -0.690762  0.625938  0.556321 -0.468636   \n",
       "796  0.144425 -0.251678 -0.103680 -0.865499 -0.878160  0.570694 -1.595752   \n",
       "797 -0.589522 -0.880361  0.773587 -0.296867 -0.271078 -0.199294 -1.703998   \n",
       "798 -0.136441  0.646560 -0.476017 -0.062668 -0.901895 -1.633245  0.811345   \n",
       "799 -0.552745 -0.270062  0.355187 -1.202716 -1.357263  1.072818  0.115408   \n",
       "\n",
       "            7         8         9  \n",
       "0    0.255613  0.981052  0.354788  \n",
       "1   -0.083420 -0.033995 -0.696346  \n",
       "2    0.043872 -0.855598 -0.737397  \n",
       "3    0.903619  0.502316 -0.904222  \n",
       "4    1.084258  0.246785  0.879446  \n",
       "..        ...       ...       ...  \n",
       "795  0.207434 -0.181016 -0.913800  \n",
       "796  0.051418 -0.625263 -0.956271  \n",
       "797  1.282618  1.047622  2.104883  \n",
       "798  0.688448  0.797758  0.536947  \n",
       "799  0.594687  1.557900  1.272627  \n",
       "\n",
       "[800 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3492494f-9296-4c6e-b99b-6b789adfeb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the order of the samples the same to map back on the\n",
    "# Use TCGA labels as a proxy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3994eeb1-972a-4f11-b093-1d7db446824d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.74678965, 0.91327356, 0.29944317, ..., 0.73152156, 0.73856296,\n",
       "        0.64872896],\n",
       "       [0.42702927, 0.33032489, 0.49930979, ..., 0.59468616, 0.25594968,\n",
       "        0.49775591],\n",
       "       [0.37808867, 0.52139799, 0.51435171, ..., 0.45008391, 0.60017762,\n",
       "        0.35836613],\n",
       "       ...,\n",
       "       [0.41375141, 0.52002896, 0.39327986, ..., 0.62038482, 0.65482385,\n",
       "        0.45506501],\n",
       "       [0.62531317, 0.87386112, 0.34198686, ..., 0.69795355, 0.73965887,\n",
       "        0.77668578],\n",
       "       [0.59298449, 0.65442758, 0.67669512, ..., 0.653978  , 0.6337722 ,\n",
       "        0.53203201]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc4902ba-d280-4a82-be17-1069401c81fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.746, 0.913, 0.299, ..., 0.731, 0.738, 0.648],\n",
       "       [0.427, 0.33 , 0.499, ..., 0.594, 0.255, 0.497],\n",
       "       [0.378, 0.521, 0.514, ..., 0.45 , 0.6  , 0.358],\n",
       "       ...,\n",
       "       [0.413, 0.52 , 0.393, ..., 0.62 , 0.654, 0.455],\n",
       "       [0.625, 0.873, 0.341, ..., 0.697, 0.739, 0.776],\n",
       "       [0.592, 0.654, 0.676, ..., 0.653, 0.633, 0.532]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83ebe01a-f4ee-4199-9c03-e88b77697068",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000,\n",
    "                           n_features=100,\n",
    "                           n_informative=10,\n",
    "                           n_redundant=90,\n",
    "                           random_state=1,\n",
    "                           shift = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1080dc35-c726-49da-a659-ffbbe2b1cc90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[35.83689773, 39.48106626, 24.07085253, ..., 35.37136211,\n",
       "        35.7226078 , 30.51555388],\n",
       "       [27.28218758, 24.31671149, 29.16569292, ..., 31.25532969,\n",
       "        23.32282631, 27.44610943],\n",
       "       [25.97285531, 29.28713243, 29.54912945, ..., 26.90566869,\n",
       "        32.16707373, 24.61216514],\n",
       "       ...,\n",
       "       [26.92695835, 29.25151945, 26.46286269, ..., 32.02834985,\n",
       "        33.57109914, 26.57815753],\n",
       "       [32.58697636, 38.45582302, 25.15534229, ..., 34.36163077,\n",
       "        35.75076495, 33.11705391],\n",
       "       [31.72207094, 32.74765656, 33.68745793, ..., 33.03883854,\n",
       "        33.03021905, 28.14297938]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " X # does each row correspond to a one or zero labe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c4b5b88-830f-485f-b047-b4c1d34f642f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "244ff9ca-268e-4c37-8e13-f92dfe91b111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda472c9-82f2-4e35-807c-9a723feae2a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
