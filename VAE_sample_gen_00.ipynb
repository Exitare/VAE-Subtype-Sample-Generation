{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c329fd-e24f-4271-b29b-2886e285ed56",
   "metadata": {
    "tags": []
   },
   "source": [
    "# VAE sample generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "250b7ff3-dc25-4049-9485-fec9565767f0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5v/88tvwqd94gs4dfr3h6rt1_w0qd13bg/T/ipykernel_19558/1053436268.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sample_gen\"\u001b[0m \u001b[0;31m# The prefix used for result files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwithout_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Tuple\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "eight_by_five = pd.read_csv(\n",
    "    '/Users/karlberb/work/sample_count/smpl_gen/architechture/X_intersect_5k_mad_feats_cohorts_n25.tsv',\n",
    "    sep = '\\t', index_col = 0)\n",
    "\n",
    "def normalize(data: pd.DataFrame, features: list, feature_range: Tuple = None, scaler=None) -> (pd.DataFrame, any):\n",
    "        if feature_range is not None and scaler is None:\n",
    "            scaler = MinMaxScaler(feature_range=feature_range)\n",
    "            scaler.fit(data)\n",
    "        elif feature_range is None and scaler is None:\n",
    "            scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "            scaler.fit(data)\n",
    "\n",
    "        data = scaler.transform(data)\n",
    "\n",
    "        return pd.DataFrame(columns=features, data=data), scaler\n",
    "    \n",
    "def create_splits(input_data: pd.DataFrame, without_val: bool = False) -> Tuple:\n",
    "        \"\"\"\n",
    "        Creates train val test split of the data provided\n",
    "        @param input_data: The input data which should be split\n",
    "        @param without_val: If true only a test and a train set will be created\n",
    "        @return: A tuple containing all the sets.\n",
    "        \"\"\"\n",
    "\n",
    "        # No validation set will be created\n",
    "        if without_val:\n",
    "            return train_test_split(input_data, test_size=0.2, random_state=1, shuffle=True)\n",
    "\n",
    "        # Create validation set\n",
    "        X_dev, X_val = train_test_split(input_data, test_size=0.05, random_state=1, shuffle=True)\n",
    "        X_train, X_test = train_test_split(X_dev, test_size=0.25, random_state=1, shuffle=True)\n",
    "        return X_train, X_val, X_test\n",
    "    \n",
    "class Sampling(layers.Layer): # inheriting\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs): # probably a tuple\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs): # **kwargs ?\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        \n",
    "        self.encoder: Model = encoder\n",
    "        self.decoder: Model = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property # wrap methods as variable, for interactive, has getter (no setter)\n",
    "                # add example as needed (syntantic sugar) further abstraction\n",
    "                #\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape: # syntactic sugar, create a temporary instance to save memory (?)\n",
    "                                        # dispose of when out scope\n",
    "                                        # Replace with human interpretable version, try, properly closed\n",
    "                                        # finally statement, deal with error\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss_fn = keras.losses.MeanSquaredError()\n",
    "            reconstruction_loss = reconstruction_loss_fn(data, reconstruction)\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        return self.decoder(z)\n",
    "    \n",
    "latent_dim = 20 # Latent dims\n",
    "data_path = \"/Users/karlberb/work/sample_count/smpl_gen/architechture/X_intersect_5k_mad_feats_cohorts_n25.tsv\"\n",
    "# Path to the data\n",
    "epochs = 15 # The number of epochs\n",
    "batch_size = 128 # The batch size\n",
    "base_path = \"decoded_BRCA\" # The base path where results are being stored\n",
    "prefix = \"sample_gen\" # The prefix used for result files\n",
    "\n",
    "data = pd.read_csv(data_path, sep='\\t', index_col=0)\n",
    "data = np.trunc(1000 * data) / 1000\n",
    "\n",
    "train_data, val_data, test_data = create_splits(input_data=data, without_val=False)\n",
    "\n",
    "train_data, scaler = normalize(train_data, features=train_data.columns)\n",
    "val_data, _ = normalize(val_data, features=val_data.columns, scaler=scaler)\n",
    "test_data, _ = normalize(data=test_data, features=test_data.columns, scaler=scaler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9c555eb-1fe8-41e2-b79e-c7c29da24c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 20 # Latent dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c819e968-0133-47da-94c6-9c4d8bbb936b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1607aecf-41af-43a5-8a71-a02aa84fe685",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpl_3lyr = (500, 250, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc593d5-107e-4fb2-959c-f77702aa21c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpl_4lyr = (1000, 500, 250, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fefa36f-5fd1-42f4-89fa-67ab042e49e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(eight_by_five.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14c1c699-e710-443e-9886-6d81f5b58836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N:GEXP::COL1A1:1277:</th>\n",
       "      <th>N:GEXP::ALB:213:</th>\n",
       "      <th>N:GEXP::FN1:2335:</th>\n",
       "      <th>N:GEXP::GAPDH:2597:</th>\n",
       "      <th>N:GEXP::ACTB:60:</th>\n",
       "      <th>N:GEXP::KRT5:3852:</th>\n",
       "      <th>N:GEXP::FTL:2512:</th>\n",
       "      <th>N:GEXP::EEF1A1:1915:</th>\n",
       "      <th>N:GEXP::B2M:567:</th>\n",
       "      <th>N:GEXP::COL3A1:1281:</th>\n",
       "      <th>...</th>\n",
       "      <th>N:GEXP::SMURF1:57154:</th>\n",
       "      <th>N:GEXP::GNPNAT1:64841:</th>\n",
       "      <th>N:GEXP::DMXL1:1657:</th>\n",
       "      <th>N:GEXP::AKT3:10000:</th>\n",
       "      <th>N:GEXP::ProSAPiP1:9762:</th>\n",
       "      <th>N:GEXP::RANBP3:8498:</th>\n",
       "      <th>N:GEXP::SIDT2:51092:</th>\n",
       "      <th>N:GEXP::AS3MT:57412:</th>\n",
       "      <th>N:GEXP::ALOX5:240:</th>\n",
       "      <th>N:GEXP::PHF2:5253:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TCGA-OR-A5J1</th>\n",
       "      <td>485.59</td>\n",
       "      <td>1.4409</td>\n",
       "      <td>1558.60</td>\n",
       "      <td>57986.0</td>\n",
       "      <td>68807.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>13570.0</td>\n",
       "      <td>33046.0</td>\n",
       "      <td>18943.0</td>\n",
       "      <td>325.65</td>\n",
       "      <td>...</td>\n",
       "      <td>2448.60</td>\n",
       "      <td>383.77</td>\n",
       "      <td>640.73</td>\n",
       "      <td>853.03</td>\n",
       "      <td>243.04</td>\n",
       "      <td>3165.2</td>\n",
       "      <td>1013.50</td>\n",
       "      <td>133.53</td>\n",
       "      <td>90.7780</td>\n",
       "      <td>1230.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-OR-A5J2</th>\n",
       "      <td>4269.10</td>\n",
       "      <td>5.2342</td>\n",
       "      <td>20086.00</td>\n",
       "      <td>45123.0</td>\n",
       "      <td>71065.0</td>\n",
       "      <td>0.8053</td>\n",
       "      <td>59215.0</td>\n",
       "      <td>61616.0</td>\n",
       "      <td>17621.0</td>\n",
       "      <td>13904.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1632.40</td>\n",
       "      <td>525.03</td>\n",
       "      <td>801.24</td>\n",
       "      <td>678.84</td>\n",
       "      <td>501.27</td>\n",
       "      <td>1733.7</td>\n",
       "      <td>937.23</td>\n",
       "      <td>1985.00</td>\n",
       "      <td>26.5740</td>\n",
       "      <td>1115.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-OR-A5J3</th>\n",
       "      <td>1164.90</td>\n",
       "      <td>5.3325</td>\n",
       "      <td>7744.60</td>\n",
       "      <td>271000.0</td>\n",
       "      <td>73505.0</td>\n",
       "      <td>1.1850</td>\n",
       "      <td>56320.0</td>\n",
       "      <td>77490.0</td>\n",
       "      <td>29841.0</td>\n",
       "      <td>1450.50</td>\n",
       "      <td>...</td>\n",
       "      <td>1785.50</td>\n",
       "      <td>674.86</td>\n",
       "      <td>1148.30</td>\n",
       "      <td>250.04</td>\n",
       "      <td>267.22</td>\n",
       "      <td>3143.2</td>\n",
       "      <td>1407.90</td>\n",
       "      <td>16851.00</td>\n",
       "      <td>20.1450</td>\n",
       "      <td>798.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-OR-A5J5</th>\n",
       "      <td>1531.40</td>\n",
       "      <td>312.9400</td>\n",
       "      <td>17322.00</td>\n",
       "      <td>127710.0</td>\n",
       "      <td>111060.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>40738.0</td>\n",
       "      <td>40610.0</td>\n",
       "      <td>6510.5</td>\n",
       "      <td>749.03</td>\n",
       "      <td>...</td>\n",
       "      <td>1209.10</td>\n",
       "      <td>254.07</td>\n",
       "      <td>1507.40</td>\n",
       "      <td>469.40</td>\n",
       "      <td>307.51</td>\n",
       "      <td>5023.2</td>\n",
       "      <td>617.51</td>\n",
       "      <td>26278.00</td>\n",
       "      <td>61.1930</td>\n",
       "      <td>2089.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-OR-A5J6</th>\n",
       "      <td>376.11</td>\n",
       "      <td>1.1177</td>\n",
       "      <td>787.43</td>\n",
       "      <td>45653.0</td>\n",
       "      <td>52458.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>166900.0</td>\n",
       "      <td>79786.0</td>\n",
       "      <td>39353.0</td>\n",
       "      <td>415.23</td>\n",
       "      <td>...</td>\n",
       "      <td>805.25</td>\n",
       "      <td>245.90</td>\n",
       "      <td>810.34</td>\n",
       "      <td>357.67</td>\n",
       "      <td>829.90</td>\n",
       "      <td>2113.6</td>\n",
       "      <td>2382.70</td>\n",
       "      <td>15389.00</td>\n",
       "      <td>156.4800</td>\n",
       "      <td>452.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-WC-AA9E</th>\n",
       "      <td>76.65</td>\n",
       "      <td>0.8861</td>\n",
       "      <td>403.63</td>\n",
       "      <td>56811.0</td>\n",
       "      <td>69981.0</td>\n",
       "      <td>30.5720</td>\n",
       "      <td>37423.0</td>\n",
       "      <td>98720.0</td>\n",
       "      <td>16375.0</td>\n",
       "      <td>922.02</td>\n",
       "      <td>...</td>\n",
       "      <td>1122.70</td>\n",
       "      <td>1078.40</td>\n",
       "      <td>621.62</td>\n",
       "      <td>636.24</td>\n",
       "      <td>587.50</td>\n",
       "      <td>1790.4</td>\n",
       "      <td>2318.20</td>\n",
       "      <td>381.48</td>\n",
       "      <td>4.4307</td>\n",
       "      <td>1086.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-YZ-A980</th>\n",
       "      <td>1841.30</td>\n",
       "      <td>0.8220</td>\n",
       "      <td>2852.40</td>\n",
       "      <td>103200.0</td>\n",
       "      <td>44996.0</td>\n",
       "      <td>1.2330</td>\n",
       "      <td>48825.0</td>\n",
       "      <td>101630.0</td>\n",
       "      <td>29023.0</td>\n",
       "      <td>1208.40</td>\n",
       "      <td>...</td>\n",
       "      <td>815.75</td>\n",
       "      <td>254.42</td>\n",
       "      <td>886.97</td>\n",
       "      <td>958.90</td>\n",
       "      <td>3026.30</td>\n",
       "      <td>2503.9</td>\n",
       "      <td>993.33</td>\n",
       "      <td>248.25</td>\n",
       "      <td>71.9280</td>\n",
       "      <td>1382.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-YZ-A982</th>\n",
       "      <td>144.25</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>628.32</td>\n",
       "      <td>59826.0</td>\n",
       "      <td>82306.0</td>\n",
       "      <td>7.0796</td>\n",
       "      <td>73261.0</td>\n",
       "      <td>64212.0</td>\n",
       "      <td>7694.7</td>\n",
       "      <td>152.21</td>\n",
       "      <td>...</td>\n",
       "      <td>924.24</td>\n",
       "      <td>233.63</td>\n",
       "      <td>142.48</td>\n",
       "      <td>859.29</td>\n",
       "      <td>734.51</td>\n",
       "      <td>2144.2</td>\n",
       "      <td>1912.80</td>\n",
       "      <td>363.70</td>\n",
       "      <td>23.0090</td>\n",
       "      <td>1319.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-YZ-A983</th>\n",
       "      <td>490.15</td>\n",
       "      <td>0.6158</td>\n",
       "      <td>774.01</td>\n",
       "      <td>83361.0</td>\n",
       "      <td>37870.0</td>\n",
       "      <td>53.5710</td>\n",
       "      <td>32539.0</td>\n",
       "      <td>348050.0</td>\n",
       "      <td>10348.0</td>\n",
       "      <td>592.98</td>\n",
       "      <td>...</td>\n",
       "      <td>1221.20</td>\n",
       "      <td>489.53</td>\n",
       "      <td>900.25</td>\n",
       "      <td>364.53</td>\n",
       "      <td>742.61</td>\n",
       "      <td>1456.9</td>\n",
       "      <td>1181.20</td>\n",
       "      <td>100.98</td>\n",
       "      <td>21.5520</td>\n",
       "      <td>942.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-YZ-A985</th>\n",
       "      <td>1777.50</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1118.50</td>\n",
       "      <td>249740.0</td>\n",
       "      <td>54427.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>40780.0</td>\n",
       "      <td>153050.0</td>\n",
       "      <td>21848.0</td>\n",
       "      <td>1128.70</td>\n",
       "      <td>...</td>\n",
       "      <td>1381.70</td>\n",
       "      <td>264.30</td>\n",
       "      <td>226.87</td>\n",
       "      <td>552.42</td>\n",
       "      <td>205.31</td>\n",
       "      <td>4059.8</td>\n",
       "      <td>1299.80</td>\n",
       "      <td>82.42</td>\n",
       "      <td>7.9403</td>\n",
       "      <td>1137.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8009 rows Ã— 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              N:GEXP::COL1A1:1277:  N:GEXP::ALB:213:  N:GEXP::FN1:2335:  \\\n",
       "TCGA-OR-A5J1                485.59            1.4409            1558.60   \n",
       "TCGA-OR-A5J2               4269.10            5.2342           20086.00   \n",
       "TCGA-OR-A5J3               1164.90            5.3325            7744.60   \n",
       "TCGA-OR-A5J5               1531.40          312.9400           17322.00   \n",
       "TCGA-OR-A5J6                376.11            1.1177             787.43   \n",
       "...                            ...               ...                ...   \n",
       "TCGA-WC-AA9E                 76.65            0.8861             403.63   \n",
       "TCGA-YZ-A980               1841.30            0.8220            2852.40   \n",
       "TCGA-YZ-A982                144.25            0.0000             628.32   \n",
       "TCGA-YZ-A983                490.15            0.6158             774.01   \n",
       "TCGA-YZ-A985               1777.50            0.0000            1118.50   \n",
       "\n",
       "              N:GEXP::GAPDH:2597:  N:GEXP::ACTB:60:  N:GEXP::KRT5:3852:  \\\n",
       "TCGA-OR-A5J1              57986.0           68807.0              0.0000   \n",
       "TCGA-OR-A5J2              45123.0           71065.0              0.8053   \n",
       "TCGA-OR-A5J3             271000.0           73505.0              1.1850   \n",
       "TCGA-OR-A5J5             127710.0          111060.0              0.0000   \n",
       "TCGA-OR-A5J6              45653.0           52458.0              0.0000   \n",
       "...                           ...               ...                 ...   \n",
       "TCGA-WC-AA9E              56811.0           69981.0             30.5720   \n",
       "TCGA-YZ-A980             103200.0           44996.0              1.2330   \n",
       "TCGA-YZ-A982              59826.0           82306.0              7.0796   \n",
       "TCGA-YZ-A983              83361.0           37870.0             53.5710   \n",
       "TCGA-YZ-A985             249740.0           54427.0              0.0000   \n",
       "\n",
       "              N:GEXP::FTL:2512:  N:GEXP::EEF1A1:1915:  N:GEXP::B2M:567:  \\\n",
       "TCGA-OR-A5J1            13570.0               33046.0           18943.0   \n",
       "TCGA-OR-A5J2            59215.0               61616.0           17621.0   \n",
       "TCGA-OR-A5J3            56320.0               77490.0           29841.0   \n",
       "TCGA-OR-A5J5            40738.0               40610.0            6510.5   \n",
       "TCGA-OR-A5J6           166900.0               79786.0           39353.0   \n",
       "...                         ...                   ...               ...   \n",
       "TCGA-WC-AA9E            37423.0               98720.0           16375.0   \n",
       "TCGA-YZ-A980            48825.0              101630.0           29023.0   \n",
       "TCGA-YZ-A982            73261.0               64212.0            7694.7   \n",
       "TCGA-YZ-A983            32539.0              348050.0           10348.0   \n",
       "TCGA-YZ-A985            40780.0              153050.0           21848.0   \n",
       "\n",
       "              N:GEXP::COL3A1:1281:  ...  N:GEXP::SMURF1:57154:  \\\n",
       "TCGA-OR-A5J1                325.65  ...                2448.60   \n",
       "TCGA-OR-A5J2              13904.00  ...                1632.40   \n",
       "TCGA-OR-A5J3               1450.50  ...                1785.50   \n",
       "TCGA-OR-A5J5                749.03  ...                1209.10   \n",
       "TCGA-OR-A5J6                415.23  ...                 805.25   \n",
       "...                            ...  ...                    ...   \n",
       "TCGA-WC-AA9E                922.02  ...                1122.70   \n",
       "TCGA-YZ-A980               1208.40  ...                 815.75   \n",
       "TCGA-YZ-A982                152.21  ...                 924.24   \n",
       "TCGA-YZ-A983                592.98  ...                1221.20   \n",
       "TCGA-YZ-A985               1128.70  ...                1381.70   \n",
       "\n",
       "              N:GEXP::GNPNAT1:64841:  N:GEXP::DMXL1:1657:  \\\n",
       "TCGA-OR-A5J1                  383.77               640.73   \n",
       "TCGA-OR-A5J2                  525.03               801.24   \n",
       "TCGA-OR-A5J3                  674.86              1148.30   \n",
       "TCGA-OR-A5J5                  254.07              1507.40   \n",
       "TCGA-OR-A5J6                  245.90               810.34   \n",
       "...                              ...                  ...   \n",
       "TCGA-WC-AA9E                 1078.40               621.62   \n",
       "TCGA-YZ-A980                  254.42               886.97   \n",
       "TCGA-YZ-A982                  233.63               142.48   \n",
       "TCGA-YZ-A983                  489.53               900.25   \n",
       "TCGA-YZ-A985                  264.30               226.87   \n",
       "\n",
       "              N:GEXP::AKT3:10000:  N:GEXP::ProSAPiP1:9762:  \\\n",
       "TCGA-OR-A5J1               853.03                   243.04   \n",
       "TCGA-OR-A5J2               678.84                   501.27   \n",
       "TCGA-OR-A5J3               250.04                   267.22   \n",
       "TCGA-OR-A5J5               469.40                   307.51   \n",
       "TCGA-OR-A5J6               357.67                   829.90   \n",
       "...                           ...                      ...   \n",
       "TCGA-WC-AA9E               636.24                   587.50   \n",
       "TCGA-YZ-A980               958.90                  3026.30   \n",
       "TCGA-YZ-A982               859.29                   734.51   \n",
       "TCGA-YZ-A983               364.53                   742.61   \n",
       "TCGA-YZ-A985               552.42                   205.31   \n",
       "\n",
       "              N:GEXP::RANBP3:8498:  N:GEXP::SIDT2:51092:  \\\n",
       "TCGA-OR-A5J1                3165.2               1013.50   \n",
       "TCGA-OR-A5J2                1733.7                937.23   \n",
       "TCGA-OR-A5J3                3143.2               1407.90   \n",
       "TCGA-OR-A5J5                5023.2                617.51   \n",
       "TCGA-OR-A5J6                2113.6               2382.70   \n",
       "...                            ...                   ...   \n",
       "TCGA-WC-AA9E                1790.4               2318.20   \n",
       "TCGA-YZ-A980                2503.9                993.33   \n",
       "TCGA-YZ-A982                2144.2               1912.80   \n",
       "TCGA-YZ-A983                1456.9               1181.20   \n",
       "TCGA-YZ-A985                4059.8               1299.80   \n",
       "\n",
       "              N:GEXP::AS3MT:57412:  N:GEXP::ALOX5:240:  N:GEXP::PHF2:5253:  \n",
       "TCGA-OR-A5J1                133.53             90.7780             1230.10  \n",
       "TCGA-OR-A5J2               1985.00             26.5740             1115.70  \n",
       "TCGA-OR-A5J3              16851.00             20.1450              798.70  \n",
       "TCGA-OR-A5J5              26278.00             61.1930             2089.10  \n",
       "TCGA-OR-A5J6              15389.00            156.4800              452.12  \n",
       "...                            ...                 ...                 ...  \n",
       "TCGA-WC-AA9E                381.48              4.4307             1086.00  \n",
       "TCGA-YZ-A980                248.25             71.9280             1382.70  \n",
       "TCGA-YZ-A982                363.70             23.0090             1319.50  \n",
       "TCGA-YZ-A983                100.98             21.5520              942.73  \n",
       "TCGA-YZ-A985                 82.42              7.9403             1137.70  \n",
       "\n",
       "[8009 rows x 5000 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eight_by_five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d20bf167-e0a9-4306-9300-08734a5d9d3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-02 11:50:24.950294: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 5000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 500)          2500500     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 250)          125250      dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 50)           12550       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 20)           1020        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 20)           1020        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sampling (Sampling)             (None, 20)           0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,640,340\n",
      "Trainable params: 2,640,340\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 20)]              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 500)               10500     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5000)              2505000   \n",
      "=================================================================\n",
      "Total params: 2,515,500\n",
      "Trainable params: 2,515,500\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-02 11:50:25.618483: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "45/45 [==============================] - 3s 39ms/step - loss: 9.7153 - reconstruction_loss: 0.7028 - kl_loss: 2.8178 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 2/15\n",
      "45/45 [==============================] - 1s 32ms/step - loss: 0.6992 - reconstruction_loss: 0.6986 - kl_loss: 5.2757e-04 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 3/15\n",
      "45/45 [==============================] - 2s 38ms/step - loss: 0.6989 - reconstruction_loss: 0.6986 - kl_loss: 3.7671e-04 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 4/15\n",
      "45/45 [==============================] - 1s 33ms/step - loss: 0.6988 - reconstruction_loss: 0.6986 - kl_loss: 2.5485e-04 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 5/15\n",
      "45/45 [==============================] - 2s 34ms/step - loss: 0.6986 - reconstruction_loss: 0.6987 - kl_loss: 1.6958e-04 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 6/15\n",
      "45/45 [==============================] - 2s 35ms/step - loss: 0.6990 - reconstruction_loss: 0.6987 - kl_loss: 1.1314e-04 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 7/15\n",
      "45/45 [==============================] - 2s 37ms/step - loss: 0.6985 - reconstruction_loss: 0.6986 - kl_loss: 7.6591e-05 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "input_dimensions = train_data.shape[1]\n",
    "\n",
    "# features count in first\n",
    "encoder_inputs = keras.Input(shape=(input_dimensions,)) # <--- Please explain\n",
    "\n",
    "x = layers.Dense(units=500, activation=\"relu\")(encoder_inputs)\n",
    "x = layers.Dense(units=250, activation=\"relu\")(x)\n",
    "x = layers.Dense(units=50, activation=\"relu\")(x)\n",
    "\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var]) # Sampling\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()\n",
    "\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(units=50, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Dense(units=250, activation=\"relu\")(x)\n",
    "x = layers.Dense(units=500, activation=\"relu\")(latent_inputs)\n",
    "\n",
    "decoder_outputs = layers.Dense(units=input_dimensions, activation=\"relu\")(x) # \n",
    "\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()\n",
    "\n",
    "vae: VAE = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam()) # Compile\n",
    "\n",
    "# vae.summary()\n",
    "\n",
    "callbacks = []\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"reconstruction_loss\",\n",
    "                           mode=\"min\", patience=5,\n",
    "                           restore_best_weights=True)\n",
    "callbacks.append(early_stop)\n",
    "\n",
    "csv_logger = CSVLogger(os.path.join(base_path, 'training.log'),\n",
    "                       separator='\\t')\n",
    "callbacks.append(csv_logger)\n",
    "\n",
    "history = vae.fit(train_data,\n",
    "                  callbacks=callbacks,\n",
    "                  validation_data=(val_data, val_data),\n",
    "                  epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "128bee46-aafb-454d-9eee-392ea38b1c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07acc5b7-06de-4a37-8451-278c871e1b43",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5v/88tvwqd94gs4dfr3h6rt1_w0qd13bg/T/ipykernel_19558/1538971556.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# plot summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val_loss'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot summary\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3064669a-f736-48aa-8eda-71aa895718a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean, z_var, embedding = vae.encoder.predict(test_data)\n",
    "embedding = pd.DataFrame(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18592976-5bd5-40f4-a345-e27e28cb2645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.131048</td>\n",
       "      <td>0.270246</td>\n",
       "      <td>-0.246083</td>\n",
       "      <td>0.041365</td>\n",
       "      <td>-0.163836</td>\n",
       "      <td>0.081337</td>\n",
       "      <td>1.413786</td>\n",
       "      <td>1.199056</td>\n",
       "      <td>-0.027455</td>\n",
       "      <td>-1.752282</td>\n",
       "      <td>0.961755</td>\n",
       "      <td>0.647381</td>\n",
       "      <td>-0.444597</td>\n",
       "      <td>-0.325811</td>\n",
       "      <td>1.549471</td>\n",
       "      <td>1.184693</td>\n",
       "      <td>0.797333</td>\n",
       "      <td>-1.082510</td>\n",
       "      <td>0.530332</td>\n",
       "      <td>-1.198620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.126688</td>\n",
       "      <td>-0.865537</td>\n",
       "      <td>0.011768</td>\n",
       "      <td>1.666962</td>\n",
       "      <td>-0.858121</td>\n",
       "      <td>0.031922</td>\n",
       "      <td>2.324522</td>\n",
       "      <td>0.294791</td>\n",
       "      <td>1.555727</td>\n",
       "      <td>0.223849</td>\n",
       "      <td>2.093488</td>\n",
       "      <td>0.902878</td>\n",
       "      <td>1.418053</td>\n",
       "      <td>0.336374</td>\n",
       "      <td>2.166124</td>\n",
       "      <td>-0.232539</td>\n",
       "      <td>-0.566460</td>\n",
       "      <td>0.805488</td>\n",
       "      <td>0.984941</td>\n",
       "      <td>0.720381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.309409</td>\n",
       "      <td>0.240684</td>\n",
       "      <td>-0.578722</td>\n",
       "      <td>0.997785</td>\n",
       "      <td>1.820078</td>\n",
       "      <td>0.279088</td>\n",
       "      <td>0.615279</td>\n",
       "      <td>-0.819495</td>\n",
       "      <td>1.053660</td>\n",
       "      <td>-1.551843</td>\n",
       "      <td>0.691233</td>\n",
       "      <td>1.401377</td>\n",
       "      <td>-0.907378</td>\n",
       "      <td>-0.511405</td>\n",
       "      <td>1.238655</td>\n",
       "      <td>-0.338208</td>\n",
       "      <td>-0.062315</td>\n",
       "      <td>1.002610</td>\n",
       "      <td>1.392565</td>\n",
       "      <td>-0.336053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.287645</td>\n",
       "      <td>0.065249</td>\n",
       "      <td>-2.163270</td>\n",
       "      <td>1.530519</td>\n",
       "      <td>-0.288061</td>\n",
       "      <td>0.780958</td>\n",
       "      <td>-0.141689</td>\n",
       "      <td>0.358015</td>\n",
       "      <td>-1.322854</td>\n",
       "      <td>-1.490353</td>\n",
       "      <td>-0.037181</td>\n",
       "      <td>0.115612</td>\n",
       "      <td>1.126498</td>\n",
       "      <td>0.561229</td>\n",
       "      <td>0.050845</td>\n",
       "      <td>-1.157329</td>\n",
       "      <td>0.498627</td>\n",
       "      <td>-0.756968</td>\n",
       "      <td>-0.585653</td>\n",
       "      <td>-1.147182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.190632</td>\n",
       "      <td>0.247284</td>\n",
       "      <td>-0.053285</td>\n",
       "      <td>-0.289804</td>\n",
       "      <td>0.387546</td>\n",
       "      <td>-0.203408</td>\n",
       "      <td>-1.908330</td>\n",
       "      <td>0.791477</td>\n",
       "      <td>-0.683180</td>\n",
       "      <td>-0.441528</td>\n",
       "      <td>1.280293</td>\n",
       "      <td>-0.496062</td>\n",
       "      <td>0.952481</td>\n",
       "      <td>-1.316954</td>\n",
       "      <td>-0.711487</td>\n",
       "      <td>1.530819</td>\n",
       "      <td>1.659239</td>\n",
       "      <td>0.212300</td>\n",
       "      <td>1.216703</td>\n",
       "      <td>-0.481534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>0.330639</td>\n",
       "      <td>1.742745</td>\n",
       "      <td>-0.808699</td>\n",
       "      <td>0.347513</td>\n",
       "      <td>-0.114090</td>\n",
       "      <td>-0.558794</td>\n",
       "      <td>-0.088584</td>\n",
       "      <td>-0.353778</td>\n",
       "      <td>0.601817</td>\n",
       "      <td>0.134081</td>\n",
       "      <td>0.234888</td>\n",
       "      <td>0.350498</td>\n",
       "      <td>1.679500</td>\n",
       "      <td>0.845552</td>\n",
       "      <td>0.645152</td>\n",
       "      <td>-1.467395</td>\n",
       "      <td>-0.836085</td>\n",
       "      <td>-1.557396</td>\n",
       "      <td>0.140161</td>\n",
       "      <td>1.063640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1898</th>\n",
       "      <td>-0.476100</td>\n",
       "      <td>-0.022002</td>\n",
       "      <td>-1.233572</td>\n",
       "      <td>-0.522810</td>\n",
       "      <td>0.608137</td>\n",
       "      <td>-0.129818</td>\n",
       "      <td>-0.744741</td>\n",
       "      <td>0.092841</td>\n",
       "      <td>0.943900</td>\n",
       "      <td>0.928072</td>\n",
       "      <td>-1.096905</td>\n",
       "      <td>-0.207279</td>\n",
       "      <td>0.263436</td>\n",
       "      <td>0.214837</td>\n",
       "      <td>0.746481</td>\n",
       "      <td>0.078662</td>\n",
       "      <td>-0.723470</td>\n",
       "      <td>-0.957757</td>\n",
       "      <td>-0.636214</td>\n",
       "      <td>0.076773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>0.166873</td>\n",
       "      <td>1.765147</td>\n",
       "      <td>-0.369428</td>\n",
       "      <td>1.228784</td>\n",
       "      <td>-0.222983</td>\n",
       "      <td>-1.079029</td>\n",
       "      <td>-0.875732</td>\n",
       "      <td>-1.523083</td>\n",
       "      <td>3.151026</td>\n",
       "      <td>-0.732707</td>\n",
       "      <td>-0.374783</td>\n",
       "      <td>-1.496104</td>\n",
       "      <td>-0.116037</td>\n",
       "      <td>-0.398805</td>\n",
       "      <td>-0.199335</td>\n",
       "      <td>-0.473715</td>\n",
       "      <td>0.119911</td>\n",
       "      <td>-0.120860</td>\n",
       "      <td>-0.057998</td>\n",
       "      <td>-1.437070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>0.082826</td>\n",
       "      <td>-2.396239</td>\n",
       "      <td>0.053919</td>\n",
       "      <td>-0.828997</td>\n",
       "      <td>0.771006</td>\n",
       "      <td>-1.277385</td>\n",
       "      <td>0.633936</td>\n",
       "      <td>-0.780836</td>\n",
       "      <td>-2.320095</td>\n",
       "      <td>-1.102198</td>\n",
       "      <td>0.414342</td>\n",
       "      <td>0.845820</td>\n",
       "      <td>1.074169</td>\n",
       "      <td>0.152199</td>\n",
       "      <td>0.418365</td>\n",
       "      <td>-1.439395</td>\n",
       "      <td>-0.249705</td>\n",
       "      <td>-1.376707</td>\n",
       "      <td>-0.375378</td>\n",
       "      <td>-0.434578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <td>-1.088116</td>\n",
       "      <td>-1.552023</td>\n",
       "      <td>-1.187581</td>\n",
       "      <td>0.146690</td>\n",
       "      <td>-0.364023</td>\n",
       "      <td>1.017989</td>\n",
       "      <td>-1.854845</td>\n",
       "      <td>0.133415</td>\n",
       "      <td>0.238197</td>\n",
       "      <td>-0.492087</td>\n",
       "      <td>1.053300</td>\n",
       "      <td>-0.556693</td>\n",
       "      <td>-1.767313</td>\n",
       "      <td>2.462283</td>\n",
       "      <td>0.440542</td>\n",
       "      <td>-1.505074</td>\n",
       "      <td>1.337930</td>\n",
       "      <td>-1.030635</td>\n",
       "      <td>-0.748908</td>\n",
       "      <td>1.618801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1902 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0    -1.131048  0.270246 -0.246083  0.041365 -0.163836  0.081337  1.413786   \n",
       "1    -0.126688 -0.865537  0.011768  1.666962 -0.858121  0.031922  2.324522   \n",
       "2     2.309409  0.240684 -0.578722  0.997785  1.820078  0.279088  0.615279   \n",
       "3    -0.287645  0.065249 -2.163270  1.530519 -0.288061  0.780958 -0.141689   \n",
       "4     1.190632  0.247284 -0.053285 -0.289804  0.387546 -0.203408 -1.908330   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1897  0.330639  1.742745 -0.808699  0.347513 -0.114090 -0.558794 -0.088584   \n",
       "1898 -0.476100 -0.022002 -1.233572 -0.522810  0.608137 -0.129818 -0.744741   \n",
       "1899  0.166873  1.765147 -0.369428  1.228784 -0.222983 -1.079029 -0.875732   \n",
       "1900  0.082826 -2.396239  0.053919 -0.828997  0.771006 -1.277385  0.633936   \n",
       "1901 -1.088116 -1.552023 -1.187581  0.146690 -0.364023  1.017989 -1.854845   \n",
       "\n",
       "            7         8         9         10        11        12        13  \\\n",
       "0     1.199056 -0.027455 -1.752282  0.961755  0.647381 -0.444597 -0.325811   \n",
       "1     0.294791  1.555727  0.223849  2.093488  0.902878  1.418053  0.336374   \n",
       "2    -0.819495  1.053660 -1.551843  0.691233  1.401377 -0.907378 -0.511405   \n",
       "3     0.358015 -1.322854 -1.490353 -0.037181  0.115612  1.126498  0.561229   \n",
       "4     0.791477 -0.683180 -0.441528  1.280293 -0.496062  0.952481 -1.316954   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1897 -0.353778  0.601817  0.134081  0.234888  0.350498  1.679500  0.845552   \n",
       "1898  0.092841  0.943900  0.928072 -1.096905 -0.207279  0.263436  0.214837   \n",
       "1899 -1.523083  3.151026 -0.732707 -0.374783 -1.496104 -0.116037 -0.398805   \n",
       "1900 -0.780836 -2.320095 -1.102198  0.414342  0.845820  1.074169  0.152199   \n",
       "1901  0.133415  0.238197 -0.492087  1.053300 -0.556693 -1.767313  2.462283   \n",
       "\n",
       "            14        15        16        17        18        19  \n",
       "0     1.549471  1.184693  0.797333 -1.082510  0.530332 -1.198620  \n",
       "1     2.166124 -0.232539 -0.566460  0.805488  0.984941  0.720381  \n",
       "2     1.238655 -0.338208 -0.062315  1.002610  1.392565 -0.336053  \n",
       "3     0.050845 -1.157329  0.498627 -0.756968 -0.585653 -1.147182  \n",
       "4    -0.711487  1.530819  1.659239  0.212300  1.216703 -0.481534  \n",
       "...        ...       ...       ...       ...       ...       ...  \n",
       "1897  0.645152 -1.467395 -0.836085 -1.557396  0.140161  1.063640  \n",
       "1898  0.746481  0.078662 -0.723470 -0.957757 -0.636214  0.076773  \n",
       "1899 -0.199335 -0.473715  0.119911 -0.120860 -0.057998 -1.437070  \n",
       "1900  0.418365 -1.439395 -0.249705 -1.376707 -0.375378 -0.434578  \n",
       "1901  0.440542 -1.505074  1.337930 -1.030635 -0.748908  1.618801  \n",
       "\n",
       "[1902 rows x 20 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23fc85eb-5301-4e77-8ccb-c722129eed21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decoded_BRCA'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4780335c-c022-4e1c-aed9-26f7f1af7945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5000*500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85cc1d5-b670-4a83-8a27-fa6eb324e484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64bd311c-34e0-4810-b899-7107caf4752e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 5000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 1000)         5001000     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 500)          500500      dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 100)          50100       dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 500)          50500       dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 500)          50500       dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sampling_2 (Sampling)           (None, 500)          0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,652,600\n",
      "Trainable params: 5,652,600\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 500)]             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1000)              501000    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 5000)              5005000   \n",
      "=================================================================\n",
      "Total params: 5,506,000\n",
      "Trainable params: 5,506,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-30 14:25:45.262843: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "45/45 [==============================] - 8s 146ms/step - loss: 21.2220 - reconstruction_loss: 0.7071 - kl_loss: 6.1285 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 2/15\n",
      "45/45 [==============================] - 6s 134ms/step - loss: 0.7052 - reconstruction_loss: 0.6986 - kl_loss: 0.0054 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 3/15\n",
      "45/45 [==============================] - 6s 133ms/step - loss: 0.7010 - reconstruction_loss: 0.6986 - kl_loss: 0.0019 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 4/15\n",
      "45/45 [==============================] - 6s 141ms/step - loss: 0.6993 - reconstruction_loss: 0.6986 - kl_loss: 6.2198e-04 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 5/15\n",
      "45/45 [==============================] - 6s 135ms/step - loss: 0.6987 - reconstruction_loss: 0.6986 - kl_loss: 2.2403e-04 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 6/15\n",
      "45/45 [==============================] - 6s 139ms/step - loss: 0.6985 - reconstruction_loss: 0.6986 - kl_loss: 9.0590e-05 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 7/15\n",
      "45/45 [==============================] - 6s 140ms/step - loss: 0.6985 - reconstruction_loss: 0.6987 - kl_loss: 4.1791e-05 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 8/15\n",
      "45/45 [==============================] - 7s 158ms/step - loss: 0.6988 - reconstruction_loss: 0.6987 - kl_loss: 2.1471e-05 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 9/15\n",
      "45/45 [==============================] - 7s 151ms/step - loss: 0.6987 - reconstruction_loss: 0.6986 - kl_loss: 1.1892e-05 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 10/15\n",
      "45/45 [==============================] - 7s 153ms/step - loss: 0.6985 - reconstruction_loss: 0.6986 - kl_loss: 6.9168e-06 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 11/15\n",
      "45/45 [==============================] - 6s 138ms/step - loss: 0.6988 - reconstruction_loss: 0.6987 - kl_loss: 4.0485e-06 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "input_dimensions = train_data.shape[1]\n",
    "\n",
    "# features count in first\n",
    "encoder_inputs = keras.Input(shape=(input_dimensions,)) # <--- Please explain\n",
    "\n",
    "x = layers.Dense(units=1000, activation=\"relu\")(encoder_inputs)\n",
    "x = layers.Dense(units=500, activation=\"relu\")(x)\n",
    "x = layers.Dense(units=100, activation=\"relu\")(x)\n",
    "\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var]) # Sampling\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()\n",
    "\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "# x = layers.Dense(units=100, activation=\"relu\")(latent_inputs)\n",
    "# x = layers.Dense(units=500, activation=\"relu\")(x)\n",
    "x = layers.Dense(units=1000, activation=\"relu\")(latent_inputs)\n",
    "\n",
    "decoder_outputs = layers.Dense(units=input_dimensions, activation=\"relu\")(x) # \n",
    "\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()\n",
    "\n",
    "vae: VAE = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "# vae.summary()\n",
    "\n",
    "callbacks = []\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"reconstruction_loss\",\n",
    "                           mode=\"min\", patience=5,\n",
    "                           restore_best_weights=True)\n",
    "callbacks.append(early_stop)\n",
    "\n",
    "csv_logger = CSVLogger(os.path.join(base_path, 'training.log'),\n",
    "                       separator='\\t')\n",
    "callbacks.append(csv_logger)\n",
    "\n",
    "history = vae.fit(train_data,\n",
    "                  callbacks=callbacks,\n",
    "                  validation_data=(val_data, val_data),\n",
    "                  epochs=epochs, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eea0a1d3-dacd-4cf1-b4b3-e893d3cb1aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean, z_var, embedding = vae.encoder.predict(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66a9ad37-d5e6-4326-b58f-a0d290128193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.1393836 ,  0.37125504, -0.9112674 , ...,  0.6503524 ,\n",
       "        -0.8516645 ,  1.1269784 ],\n",
       "       [ 1.8653744 , -0.6948046 , -0.50953454, ...,  0.6189342 ,\n",
       "        -1.5743269 , -0.26229087],\n",
       "       [ 0.45815405,  1.0232786 , -1.3403239 , ..., -0.7619574 ,\n",
       "        -1.8295772 , -2.6909783 ],\n",
       "       ...,\n",
       "       [ 0.87550706, -1.2188705 ,  0.7291777 , ..., -0.962931  ,\n",
       "         2.661737  ,  1.3980938 ],\n",
       "       [ 1.6765723 , -0.27009642, -1.3959305 , ..., -0.06532443,\n",
       "         1.9460304 ,  0.46898675],\n",
       "       [-0.7611093 ,  0.04697263,  0.5281616 , ...,  1.5470366 ,\n",
       "         0.9485749 ,  0.2517841 ]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b0431ce-c678-49c7-b554-38d34ec329aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.predict(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee033cc-24a7-4612-ae58-60ee79c02e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c93ee9b-f7e8-4f9f-87dc-7bd46bcefbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae: VAE = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f24c727f-732d-47b3-9971-baee7f96f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15 # The number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5e635c-97c3-445a-b977-27ddf3ed61dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = vae.fit(train_data,\n",
    "                  callbacks=callbacks,\n",
    "                  validation_data=(val_data, val_data),\n",
    "                  epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a53203bd-1334-4884-a389-f69e33d7ca52",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "45/45 [==============================] - 22s 471ms/step - loss: 0.6995 - reconstruction_loss: 0.6986 - kl_loss: 2.2449e-04 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 2/15\n",
      "45/45 [==============================] - 20s 455ms/step - loss: 0.6985 - reconstruction_loss: 0.6987 - kl_loss: 8.4374e-07 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 3/15\n",
      "45/45 [==============================] - 21s 465ms/step - loss: 0.6986 - reconstruction_loss: 0.6986 - kl_loss: 0.0000e+00 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 4/15\n",
      "45/45 [==============================] - 21s 459ms/step - loss: 0.6986 - reconstruction_loss: 0.6986 - kl_loss: 0.0000e+00 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 5/15\n",
      "45/45 [==============================] - 21s 464ms/step - loss: 0.6986 - reconstruction_loss: 0.6986 - kl_loss: 0.0000e+00 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 6/15\n",
      "45/45 [==============================] - 21s 467ms/step - loss: 0.6986 - reconstruction_loss: 0.6986 - kl_loss: 0.0000e+00 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 7/15\n",
      "45/45 [==============================] - 21s 466ms/step - loss: 0.6985 - reconstruction_loss: 0.6986 - kl_loss: 0.0000e+00 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 8/15\n",
      "45/45 [==============================] - 21s 463ms/step - loss: 0.6987 - reconstruction_loss: 0.6986 - kl_loss: 0.0000e+00 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 9/15\n",
      "45/45 [==============================] - 21s 461ms/step - loss: 0.6986 - reconstruction_loss: 0.6986 - kl_loss: 0.0000e+00 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 10/15\n",
      "45/45 [==============================] - 22s 482ms/step - loss: 0.6987 - reconstruction_loss: 0.6986 - kl_loss: 0.0000e+00 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 11/15\n",
      "45/45 [==============================] - 24s 528ms/step - loss: 0.6987 - reconstruction_loss: 0.6986 - kl_loss: 0.0000e+00 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = vae.fit(train_data,\n",
    "                  callbacks=callbacks,\n",
    "                  validation_data=(val_data, val_data),\n",
    "                  epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e505f2b-a6da-4cef-8e7b-98306619b05e",
   "metadata": {},
   "source": [
    "### Create embeddings for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a2320d7-d30e-4c45-9b34-f8dca46ffe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean, z_var, embedding = vae.encoder.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65ab1ac0-2b4b-4b62-b817-8615a0549a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.9870646 ,  1.0416583 , -0.7838087 , ..., -0.53242105,\n",
       "        -0.7526022 , -0.39004812],\n",
       "       [ 0.34480008, -1.2102859 ,  0.516639  , ..., -0.17086384,\n",
       "        -0.87621814, -0.41115296],\n",
       "       [ 0.9989898 , -1.5573266 ,  1.3649637 , ..., -1.0215573 ,\n",
       "        -0.10033275,  1.0939907 ],\n",
       "       ...,\n",
       "       [-1.1655518 , -0.5408735 ,  0.41010308, ...,  0.9028942 ,\n",
       "         1.2008036 ,  1.0852925 ],\n",
       "       [ 2.0437467 ,  1.3344986 ,  0.0473986 , ..., -1.1208513 ,\n",
       "         0.86585915,  1.5747652 ],\n",
       "       [-0.9168553 , -1.058427  ,  0.9859297 , ..., -0.4353223 ,\n",
       "         1.3822342 , -0.11381175]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "93e7f12e-8a84-47e8-aa97-b649cf3300f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.predict(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f121221-f480-4063-b8e2-c90ddde9a659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef90d78-63cf-4265-9e12-4d45c8326c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean, z_var, embedding = vae.encoder.predict(test_data)\n",
    "embedding = pd.DataFrame(embedding)\n",
    "embedding.to_csv(Path(base_path, f\"{prefix}_embeddings.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
